\chapter{Implementation}
\label{ch:impl}
\markright{Implementation}

%
\newarchitectureoverview{}

Figure~\ref{fig:cb2arch} shows the design of \cbtwo, which consists of a single
master process, multiple reverse proxies, and multiple worker processes.
All processes communicate via standard TCP/IP sockets, 
so they can be located on a shared multiprocessor machine or in a cluster.
The master is responsible for application management and request dispatch logic.
The workers host virtual browsers and serves user requests.
The reverse proxies are child processes of master responsible for the actual
dispatch job: copying messages back and forth from users to workers.
When a worker process starts up, it will contact the master to register itself
and fetch applications' metadata.
Then it will initialize application logic and listens on a HTTP port for user requests.
The master dispatches the user requests to workers through reverse proxies.
As in Figure~\ref{fig:webscaleout}, the master process is equivalent to the load balancer
and the workers are the web layer.

\section{Master}


\subsection{Request Dispatch}
To preserve the semantics of 
Application Instance(see section~\ref{sec:appins}),
each application instance and its virtual browsers need to be allocated in
the same worker process as emphasized in Figure~\ref{fig:appidhierarchy},
so the requests associated with the same application instance needs
to be dispatched to the worker that has that application instance.
We implemented the request dispatch algorithm 
to dispatch the requests in application instance level.
The master process maintains a 
table of which worker is responsible for which application instance.
Multiple reverse proxy processes are bound to the socket that accepts client requests, allowing
the OS to distribute pending client connections in a round-robin fashion.  
When a proxy process accepts a client, it parses the incoming HTTP request 
to extract the application instance id, which is part of the request URL.  
If the application instance id is already in the master's table, the request is directly
relayed to the corresponding worker.  
Otherwise, the master finds a worker using a load balancing algorithm(described in Section~\ref{sec:lb})
, the worker will then either create a new application instance 
or redirect the request to an existing application instance
or reject the request based on application specific settings.  
The reverse proxy can 
relay both HTTP requests/responses as well as the bidirectional web socket protocol after the
connection has been upgraded.  Once established, the majority of traffic will be web socket
messages for which there is relatively little per-message overhead.  We implemented the proxy 
using the \nodejs{} module note-http-proxy~\cite{nodeproxy}.


\subsection{Load Balancing}
\label{sec:lb}
When a new application instance is going to be created,
the master will apply a load balancing algorithm to decide on which worker to place this instance.  
We support two load balancing strategies: first, the master can assign application instances
to workers in a simple round-robin fashion.  
However, since application instances may vary
widely in terms of the actual cost they impose on a worker, we also implemented a load-based
scheme in which workers periodically report a measure of current load to the master.
The master will select the worker with the lowest load when placing a new application instance.
We have found the amount of heap memory that is currently in use a good measure of a worker's 
momentary load.

In the load-based mode, it is impossible for the master to get
the load of the worker in real time.
It is possible that a burst of application instances will 
be assigned to the same worker that has the lowest load at that moment.
This could cause extremely uneven load distribution.
To mitigate this issue, the master adds an empirical value to the cached
load value of a worker after it assigns a new application instance to it.
This empirical value reflects our estimation about an application instance's 
effect on a worker.
Even under bursts of new requests, 
the load distribution between the workers would not vary greatly.


\section{Worker}

Worker processes is responsible for serving the client requests.
When a worker receives a request,
it first extracts application instance id from the request.
If the request has an application instance id
and it matches a local application instance,
then the application instance is fetched to serve the request.
If the application instance id doesn't match any local application instance,
it would responds a internal error message to the user.
If there is no application instance id 
in the request,
then the worker would handle the request based on application instantiation strategy
mentioned in section~\ref{sec:appinstantiation}.
In this situation,
the worker would either, ask master for existing application instances and
redirect the request to existing instances,
or create an new application instance for this request.



